name: ML Pipeline Tests with CML Report

on:
  push:
    branches: [ main, dev ]
    paths:
      - 'week-4/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'week-4/**'
  workflow_dispatch:

env:
  WORKING_DIR: week-4

jobs:
  test-ml-pipeline:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
      issues: write
      
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
      
    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      working-directory: ${{ env.WORKING_DIR }}
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-xdist coverage[toml]
        
    - name: Install CML
      uses: iterative/setup-cml@v1
      
    - name: Run unit tests with coverage
      working-directory: ${{ env.WORKING_DIR }}
      run: |
        echo "# ML Pipeline Test Report" > report.md
        echo "" >> report.md
        echo "Generated on: $(date)" >> report.md
        echo "" >> report.md
        
        # Run tests
        echo "## Unit Test Results" >> report.md
        echo "" >> report.md
        
        if python -m pytest test_pipeline.py -v --cov=. --cov-report=term --cov-report=xml --junit-xml=test-results.xml --tb=short; then
          echo "✅ **All tests passed successfully!**" >> report.md
        else
          echo "❌ **Some tests failed. See logs for details.**" >> report.md
        fi
        echo "" >> report.md
        
    - name: Run model training validation
      working-directory: ${{ env.WORKING_DIR }}
      run: |
        echo "## Model Training Validation" >> report.md
        echo "" >> report.md
        
        if python -c "from main import Config, IrisPipeline; config = Config(data_path='data/iris.csv'); pipeline = IrisPipeline(config); pipeline.run_training_pipeline()"; then
          echo "✅ **Model training completed successfully**" >> report.md
          
          if [ -f "artifacts/metrics.json" ]; then
            echo "📊 **Performance Metrics:**" >> report.md
            echo '```json' >> report.md
            cat artifacts/metrics.json >> report.md
            echo '```' >> report.md
          fi
        else
          echo "❌ **Model training failed**" >> report.md
        fi
        echo "" >> report.md
        
    - name: Data validation
      working-directory: ${{ env.WORKING_DIR }}
      run: |
        echo "## Data Quality Report" >> report.md
        echo "" >> report.md
        
        if [ -f "data/iris.csv" ]; then
          echo "✅ **Data file exists**: \`data/iris.csv\`" >> report.md
          rows=$(python -c "import pandas as pd; print(len(pd.read_csv('data/iris.csv')))")
          cols=$(python -c "import pandas as pd; print(len(pd.read_csv('data/iris.csv').columns))")
          echo "📊 **Dataset shape**: $rows rows, $cols columns" >> report.md
        else
          echo "❌ **Data file missing**: \`data/iris.csv\`" >> report.md
        fi
        echo "" >> report.md
        
    - name: Generate test summary
      working-directory: ${{ env.WORKING_DIR }}
      run: |
        echo "## Summary" >> report.md
        echo "" >> report.md
        
        if [ -f "test-results.xml" ]; then
          total_tests=$(python -c "import xml.etree.ElementTree as ET; tree = ET.parse('test-results.xml'); print(tree.getroot().get('tests', '0'))")
          failures=$(python -c "import xml.etree.ElementTree as ET; tree = ET.parse('test-results.xml'); print(tree.getroot().get('failures', '0'))")
          
          echo "📈 **Total Tests**: $total_tests" >> report.md
          echo "❌ **Failed Tests**: $failures" >> report.md
          
          if [ "$failures" == "0" ]; then
            echo "🎉 **All tests are passing!**" >> report.md
          else
            echo "⚠️ **Action required**: Fix failing tests" >> report.md
          fi
        fi
        
        echo "" >> report.md
        echo "---" >> report.md
        echo "*Automated report generated by GitHub Actions*" >> report.md
        
    - name: Publish CML Report
      env:
        REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      working-directory: ${{ env.WORKING_DIR }}
      run: |
        cml comment create report.md
        
    - name: Upload test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results
        path: |
          week-4/test-results.xml
          week-4/coverage.xml
          week-4/artifacts/
        retention-days: 30 